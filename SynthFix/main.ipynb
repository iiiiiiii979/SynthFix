{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import re\n",
    "import sacrebleu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm import tqdm, auto\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification, \n",
    "    pipeline, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EvalPrediction\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    SFTTrainer, \n",
    "    RewardTrainer, \n",
    "    RewardConfig, \n",
    "    PPOTrainer, \n",
    "    PPOConfig, \n",
    "    AutoModelForCausalLMWithValueHead\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from codebleu import calc_codebleu\n",
    "import bitsandbytes as bnb\n",
    "from torch.distributions import Bernoulli\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Tokenizer and Setting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# Initialize the tokenizer from a pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# Set padding token to be the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to left\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Set the device to the first CUDA device\n",
    "device = torch.device('cuda:0')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load buggy and fixed data from text files\n",
    "buggy_data = Path('./FixJS/input/50/before_tokenized.txt').read_text(encoding='utf-8').splitlines()\n",
    "fixed_data = Path('./FixJS/input/50/after_tokenized.txt').read_text(encoding='utf-8').splitlines()\n",
    "\n",
    "# Get the length of the data and create an array of indices\n",
    "data_len = len(buggy_data)\n",
    "indices = np.arange(data_len)\n",
    "\n",
    "# Shuffle the indices for randomizing the data\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Apply the shuffled indices to the data\n",
    "buggy_data = np.array(buggy_data, dtype=object)[indices].tolist()\n",
    "fixed_data = np.array(fixed_data, dtype=object)[indices].tolist()\n",
    "\n",
    "# Function to add a function name in place within the data\n",
    "def add_function_name_in_place(data, function_name):\n",
    "    for i in range(len(data)):\n",
    "        index = data[i].find('(')\n",
    "        if index != -1:\n",
    "            data[i] = data[i][:index] + function_name + ' ' + data[i][index:]\n",
    "\n",
    "# Define the function name to be added\n",
    "function_name = 'Function'\n",
    "\n",
    "# Add the function name to the buggy and fixed data\n",
    "add_function_name_in_place(buggy_data, function_name)\n",
    "add_function_name_in_place(fixed_data, function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the starting indices for validation and test sets\n",
    "valid_start = int(data_len * 0.8)\n",
    "test_start = valid_start + int(data_len * 0.1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_input, train_target = buggy_data[:valid_start], fixed_data[:valid_start]\n",
    "valid_input, valid_target = buggy_data[valid_start:test_start], fixed_data[valid_start:test_start]\n",
    "test_input, test_target = buggy_data[test_start:], fixed_data[test_start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Data for Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data and convert it into model inputs\n",
    "def process_data_to_model_inputs(buggy_data, fixed_data, device):\n",
    "    model_inputs = []\n",
    "    for buggy, fixed in zip(buggy_data, fixed_data):\n",
    "        # Tokenize buggy data\n",
    "        inputs = tokenizer(buggy, max_length=50, truncation=True, padding='max_length', return_tensors='pt').to(device)\n",
    "        # Tokenize fixed data\n",
    "        outputs = tokenizer(fixed, max_length=50, truncation=True, padding='max_length', return_tensors='pt').to(device)\n",
    "\n",
    "        # Append processed data to model inputs list\n",
    "        model_inputs.append({\n",
    "            'input_ids': inputs.input_ids.squeeze(0),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(0),\n",
    "            'labels': outputs.input_ids.squeeze(0)\n",
    "        })\n",
    "    return model_inputs\n",
    "\n",
    "# Process training, validation, and test datasets\n",
    "train_dataset = process_data_to_model_inputs(train_input, train_target, device)\n",
    "val_dataset = process_data_to_model_inputs(valid_input, valid_target, device)\n",
    "test_dataset = process_data_to_model_inputs(test_input, test_target, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculating Code Quality and AST Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Quality Score: 98\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate code quality score using Semgrep\n",
    "def calculate_code_quality_score(code, penalize_parsing_errors=False):\n",
    "    # Define the scoring system\n",
    "    max_score = 100\n",
    "    points_deduct_per_issue = {\n",
    "        \"find_eval\": 10,  \n",
    "        \"user-input-in-code\": 10,  \n",
    "        \"plaintext-sensitive-info\": 20\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Setup environment for Node.js and Semgrep\n",
    "    node_directory = '/home/user/.nvm/versions/node/v21.6.2/bin'\n",
    "    env = os.environ.copy()\n",
    "    env['PATH'] = node_directory + os.pathsep + env['PATH']\n",
    "    rule_file_js='/home/user/vul.yaml'\n",
    "\n",
    "    # Create a temporary file with the input code\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".js\", delete=False) as temp_file:\n",
    "        temp_file_path = temp_file.name\n",
    "        temp_file.write(code.encode('utf-8'))\n",
    "\n",
    "    try:\n",
    "        # Execute Semgrep on the temporary file\n",
    "        command = ['npx', 'semgrep', rule_file_js, temp_file_path, '--format=json']\n",
    "        result = subprocess.run(command, capture_output=True, text=True, env=env)\n",
    "\n",
    "        # Start with the maximum score\n",
    "        score = max_score\n",
    "\n",
    "        if result.stdout:\n",
    "            semgrep_output = json.loads(result.stdout)\n",
    "            for file_result in semgrep_output:\n",
    "                for message in file_result['messages']:\n",
    "                    # Deduct points for parsing errors specifically\n",
    "                    if 'message' in message and 'error' in message['message']:\n",
    "                        if penalize_parsing_errors:\n",
    "                            score -= 50\n",
    "                    else:\n",
    "                        rule_id = message.get('ruleId', 'default')\n",
    "                        deduction = points_deduct_per_issue.get(rule_id, points_deduct_per_issue['default'])\n",
    "                        score -= deduction\n",
    "\n",
    "            # Ensure score does not go below 0\n",
    "            score = max(0, score)\n",
    "\n",
    "    finally:\n",
    "        # Clean up by deleting the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Example calculation of code quality score\n",
    "code_quality_score = calculate_code_quality_score(\"var x = 1;\")\n",
    "print(f\"Code Quality Score: {code_quality_score}\")\n",
    "\n",
    "# Function to calculate AST reward based on Semgrep quality scores\n",
    "def ast_reward(generated_code, target_code):\n",
    "    # Calculate Semgrep-based quality scores for both pieces of code\n",
    "    generated_code_score = calculate_code_quality_score(generated_code, penalize_parsing_errors=True)\n",
    "    target_code_score = calculate_code_quality_score(target_code, penalize_parsing_errors=False)\n",
    "    alignment_penalty = abs(generated_code_score - target_code_score)\n",
    "\n",
    "    # Define the total score, possibly adjusting the weights as necessary\n",
    "    # Assuming both scores and alignment are equally important\n",
    "    total_score = (generated_code_score + (100 - alignment_penalty)) / 2\n",
    "\n",
    "    return total_score / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and Comparing Control Flow Graphs (CFGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'program': {'flowGraph': {'nodes': [{'id': 1, 'type': 'Entry'}, {'id': 2, 'type': 'SuccessExit'}], 'edges': [{'from': 1, 'to': 2, 'type': 'Epsilon', 'label': '', 'data': None}]}}, 'functions': [{'id': 1, 'name': 'Function', 'flowGraph': {'nodes': [{'id': 4, 'type': 'Entry'}, {'id': 5, 'type': 'SuccessExit'}, {'id': 7, 'type': 'Normal'}], 'edges': [{'from': 4, 'to': 7, 'type': 'Normal', 'label': 'this.socket.close()', 'data': {'type': 'CallExpression', 'callee': {'type': 'MemberExpression', 'computed': False, 'object': {'type': 'MemberExpression', 'computed': False, 'object': {'type': 'ThisExpression'}, 'property': {'type': 'Identifier', 'name': 'socket'}}, 'property': {'type': 'Identifier', 'name': 'close'}}, 'arguments': []}}, {'from': 7, 'to': 5, 'type': 'AbruptCompletion', 'label': 'return undefined', 'data': {'type': 'ReturnStatement', 'argument': {'type': 'Identifier', 'name': 'undefined'}}}]}}]}\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "def get_cfg_from_code(code):\n",
    "    # Convert the code to a format that can be passed as a command line argument\n",
    "    formatted_code = code.replace('\"', '\\\\\"')\n",
    "\n",
    "    # Path to your Node.js executable\n",
    "    node_path = '/home/user/.nvm/versions/node/v21.6.2/bin/node'\n",
    "\n",
    "    # Path to your JavaScript file that generates the CFG\n",
    "    script_path = '/home/user/PPOFixer/generate_cfg.js'\n",
    "\n",
    "    try:\n",
    "        # Execute the Node.js script and capture the output\n",
    "        result = subprocess.run([node_path, script_path, formatted_code], capture_output=True, text=True, check=True)\n",
    "\n",
    "        # Attempt to parse the output as JSON\n",
    "        cfg_or_error = json.loads(result.stdout)\n",
    "        cfg_or_error = json.loads(cfg_or_error)\n",
    "\n",
    "        # Check if the output is an error message\n",
    "        if isinstance(cfg_or_error, dict) and cfg_or_error.get('error'):\n",
    "            # Handle the syntax error or other errors reported by the Node.js script\n",
    "            return None\n",
    "        else:\n",
    "            # If there's no error, return the CFG\n",
    "            return cfg_or_error\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # This catches errors from the subprocess itself, such as if the script fails to run\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        # This catches errors in parsing the output from the script\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors\n",
    "        return None\n",
    "\n",
    "    # Return None in case of any error\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "code1 = 'function Function ( ) { this. socket. close ( ) ; }'\n",
    "code2 = 'function Function ( ) { node. socket. close ( ) ; }'\n",
    "cfg1 = get_cfg_from_code(code1)\n",
    "cfg2 = get_cfg_from_code(code2)\n",
    "print(cfg1)\n",
    "\n",
    "def operation_similarity(edge1, edge2):\n",
    "    # Ensure 'data' key exists and its value is not None and is subscriptable (e.g., a dictionary)\n",
    "    if ('data' not in edge1 or edge1['data'] is None or not isinstance(edge1['data'], dict)) or \\\n",
    "       ('data' not in edge2 or edge2['data'] is None or not isinstance(edge2['data'], dict)):\n",
    "        return False\n",
    "    \n",
    "    # After ensuring 'data' is a valid dictionary, check for 'type' key\n",
    "    if 'type' not in edge1['data'] or 'type' not in edge2['data']:\n",
    "        return False\n",
    "\n",
    "    # Proceed with the comparison if all checks pass\n",
    "    return edge1['data']['type'] == edge2['data']['type']\n",
    "\n",
    "\n",
    "def compute_node_similarity(cfg1, cfg2):\n",
    "    \n",
    "    nodes1 = {node['id']: node['type'] for node in cfg1['program']['flowGraph']['nodes']}\n",
    "    nodes2 = {node['id']: node['type'] for node in cfg2['program']['flowGraph']['nodes']}\n",
    "    common_nodes = set(nodes1.items()) & set(nodes2.items())\n",
    "    total_nodes = set(nodes1.items()) | set(nodes2.items())\n",
    "    return len(common_nodes) / len(total_nodes)\n",
    "\n",
    "def compute_edge_similarity(cfg1, cfg2):\n",
    "    \n",
    "    edges1 = cfg1['program']['flowGraph']['edges']\n",
    "    edges2 = cfg2['program']['flowGraph']['edges']\n",
    "\n",
    "    # Count how many edges in edges1 have a similar operation in edges2\n",
    "    similar_count = 0\n",
    "    for edge1 in edges1:\n",
    "        for edge2 in edges2:\n",
    "            if operation_similarity(edge1, edge2):\n",
    "                similar_count += 1\n",
    "                break  # Assuming each edge in edges1 is only compared once\n",
    "\n",
    "    # Calculate similarity score based on the number of similar operations to the total unique operations\n",
    "    total_unique_edges = len(edges1) + len(edges2) - similar_count\n",
    "    return similar_count / total_unique_edges if total_unique_edges else 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_path_similarity(cfg1, cfg2):\n",
    "   \n",
    "    path_count_cfg1 = len(cfg1['program']['flowGraph']['edges'])  # Simplified placeholder\n",
    "    path_count_cfg2 = len(cfg2['program']['flowGraph']['edges'])  # Simplified placeholder\n",
    "    \n",
    "    # Calculate similarity as inverse of difference; other metrics could be more sophisticated\n",
    "    similarity = 1 - abs(path_count_cfg1 - path_count_cfg2) / max(path_count_cfg1, path_count_cfg2)\n",
    "    return similarity\n",
    "\n",
    "def compute_structural_differences(cfg_buggy, cfg_fixed):\n",
    "\n",
    "    nodes_buggy = set((node['id'], node['type']) for node in cfg_buggy['program']['flowGraph']['nodes'])\n",
    "    nodes_fixed = set((node['id'], node['type']) for node in cfg_fixed['program']['flowGraph']['nodes'])\n",
    "    edges_buggy = set(((edge['from'], edge['to']), edge['type']) for edge in cfg_buggy['program']['flowGraph']['edges'])\n",
    "    edges_fixed = set(((edge['from'], edge['to']), edge['type']) for edge in cfg_fixed['program']['flowGraph']['edges'])\n",
    "    \n",
    "    # Count nodes and edges present in cfg_fixed but not in cfg_buggy\n",
    "    new_nodes = nodes_fixed - nodes_buggy\n",
    "    new_edges = edges_fixed - edges_buggy\n",
    "\n",
    "    # Count nodes and edges removed from cfg_buggy to cfg_fixed\n",
    "    removed_nodes = nodes_buggy - nodes_fixed\n",
    "    removed_edges = edges_buggy - edges_fixed\n",
    "\n",
    "    # The difference could be a simple sum of new and removed components\n",
    "    total_difference = len(new_nodes) + len(new_edges) + len(removed_nodes) + len(removed_edges)\n",
    "    \n",
    "    return total_difference\n",
    "\n",
    "def cfg_reward(generated_code, target_code):\n",
    "\n",
    "    cfg_generated = get_cfg_from_code(generated_code)\n",
    "    cfg_target = get_cfg_from_code(target_code)\n",
    "\n",
    "    if cfg_generated is None or cfg_target is None:\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    node_similarity = compute_node_similarity(cfg_generated, cfg_target)\n",
    "    edge_similarity = compute_edge_similarity(cfg_generated, cfg_target)\n",
    "    path_similarity = compute_path_similarity(cfg_generated, cfg_target)\n",
    "    structural_difference = compute_structural_differences(cfg_generated, cfg_target)\n",
    "    \n",
    "    # Composite score calculation example (weights can be adjusted)\n",
    "    score = (node_similarity + edge_similarity + path_similarity + 1 - structural_difference) / 4\n",
    "    return score\n",
    "\n",
    "\n",
    "print(cfg_reward(code1, code2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and Training the Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Critic Model\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  # Output a scalar value as the expected return\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Directly output the expected return without activation function\n",
    "        return x\n",
    "\n",
    "# Definition of the Environment Class\n",
    "class CodeEnvironment:\n",
    "    def calculate_reward(self, generated_code, target_code):\n",
    "        quality_score = calculate_code_quality_score(generated_code)\n",
    "        cfg_score = cfg_reward(generated_code, target_code)\n",
    "        return (quality_score + cfg_score) / 2\n",
    "\n",
    "# Load model and tokenizer\n",
    "actor_model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-350m-nl', trust_remote_code=True)\n",
    "\n",
    "# Set the device\n",
    "actor_model = actor_model.to(device)\n",
    "critic = CriticModel(input_dim=50, hidden_dim=256).to(device)  # Adjust input dimensions as needed\n",
    "optimizer = optim.Adam(critic.parameters(), lr=0.01)\n",
    "environment = CodeEnvironment()\n",
    "\n",
    "# Training data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=400, shuffle=True)\n",
    "\n",
    "# Define training loop\n",
    "num_epochs = 3  # Adjust the number of epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Use the actor model to generate code\n",
    "        generated_ids = actor_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50  # Allowable number of additional tokens\n",
    "        )\n",
    "        \n",
    "        # Convert generated IDs to code text\n",
    "        generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Convert label IDs to target code text\n",
    "        target_code = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = environment.calculate_reward(generated_code, target_code)\n",
    "\n",
    "        # Expand reward tensor to match the size of predicted_value\n",
    "        reward_tensor = torch.full((input_ids.size(0),), reward, device=device)\n",
    "\n",
    "        # Get the expected return from the critic\n",
    "        predicted_value = critic(input_ids.float())  # Ensure the input type is correct\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(predicted_value.squeeze(), reward_tensor)\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the critic model\n",
    "torch.save(critic.state_dict(), 'critic_model.pth')\n",
    "print(\"Critic model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Router Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMethodSelector(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64):\n",
    "        super(TrainingMethodSelector, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with PPO and SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Critic Model\n",
    "\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  # Output a scalar value as the expected return\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Directly output the expected return without activation function\n",
    "        return x\n",
    "\n",
    "# Define reward calculation function\n",
    "def calculate_rewards(policy_output, labels, critic_model, tokenizer):\n",
    "    generated_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in policy_output]\n",
    "    target_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in labels]\n",
    "    \n",
    "    rewards = []\n",
    "    for gen_text, target_text in zip(generated_texts, target_texts):\n",
    "        quality_score = calculate_code_quality_score(gen_text)\n",
    "        cfg_score = cfg_reward(gen_text, target_text)\n",
    "        \n",
    "        # Encode the generated text to token IDs\n",
    "        gen_text_ids = tokenizer.encode(gen_text, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Ensure gen_text_ids shape matches CriticModel's input expectations\n",
    "        if gen_text_ids.shape[1] < critic_model.fc1.in_features:\n",
    "            padding = torch.zeros(1, critic_model.fc1.in_features - gen_text_ids.shape[1]).to(device)\n",
    "            gen_text_ids = torch.cat((gen_text_ids, padding), dim=1)\n",
    "        elif gen_text_ids.shape[1] > critic_model.fc1.in_features:\n",
    "            gen_text_ids = gen_text_ids[:, :critic_model.fc1.in_features]\n",
    "        \n",
    "        critic_score = critic_model(gen_text_ids.float())\n",
    "        total_reward = (-quality_score - cfg_score + critic_score.item()) / 3\n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return torch.tensor(rewards).float().to(device)\n",
    "\n",
    "# Define PPO step function\n",
    "def ppo_step(model, inputs, old_log_probs, values, rewards, advantages, optimizer, clip_param=0.2):\n",
    "    # Forward pass to get new log_probs and value estimates\n",
    "    outputs = model(**inputs)\n",
    "    new_log_probs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Match shapes of new_log_probs and old_log_probs\n",
    "    new_log_probs = new_log_probs.view_as(old_log_probs)\n",
    "\n",
    "    # Calculate probability ratios\n",
    "    ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Ensure advantages shape matches ratios shape\n",
    "    advantages = advantages.unsqueeze(-1).expand_as(ratios)\n",
    "\n",
    "    # Calculate the clipped objective function\n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    # Calculate value loss (e.g., mean squared error loss)\n",
    "    value_loss = F.mse_loss(values, rewards.unsqueeze(-1))\n",
    "\n",
    "    # Total loss\n",
    "    loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", load_in_4bit=True, quantization_config=None)\n",
    "#model.to(device)\n",
    "\n",
    "# Freeze most parameters\n",
    "'''for name, parameter in model.named_parameters():\n",
    "    if not (\"decoder.block.4\" in name or \"decoder.block.5\" in name or \"lm_head\" in name):\n",
    "        parameter.requires_grad = False\n",
    "'''\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not ((\"model.layers.28\" in name or \n",
    "             \"model.layers.29\" in name or \n",
    "             \"model.layers.30\" in name or \n",
    "             \"model.layers.31\" in name or \n",
    "             \"lm_head\" in name)):\n",
    "        parameter.requires_grad = False\n",
    "# Load critic model\n",
    "critic_model = CriticModel(input_dim=512, hidden_dim=256).to(device)  # Adjust input_dim as per actual data\n",
    "\n",
    "critic_model.to(device)\n",
    "\n",
    "# Define data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduce batch size\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Initialize the controller network and its optimizer\n",
    "training_method_selector = TrainingMethodSelector(input_size=5, hidden_size=64).to(device)\n",
    "controller_optimizer = torch.optim.Adam(training_method_selector.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "ppo_max_steps = 3  # Maximum number of PPO training steps allowed\n",
    "ppo_steps = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Baseline loss for reward calculation and moving average smoothing coefficient\n",
    "baseline_loss = 1.0  \n",
    "alpha = 0.9  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_dataloader)\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        # Compute feature vector for the current batch\n",
    "        features = compute_batch_features(batch)  # shape: [1, 5]\n",
    "        \n",
    "        # Controller network outputs decision probability\n",
    "        prob = training_method_selector(features)  # Output shape: [1,1]\n",
    "        m = Bernoulli(prob)\n",
    "        action = m.sample()  # 0 for SFT training, 1 for PPO training\n",
    "        log_prob = m.log_prob(action)\n",
    "        \n",
    "        # Select training method based on controller decision\n",
    "        if action.item() == 1 and ppo_steps < ppo_max_steps:\n",
    "            print(\"PPO training for this batch\")\n",
    "            # PPO training branch\n",
    "            # Move all batch data (except 'code') to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != 'code'}\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            old_log_probs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "            rewards = calculate_rewards(predictions, batch['labels'], critic_model, tokenizer)\n",
    "            \n",
    "            # Adjust input_ids shape to match the CriticModel's input requirements\n",
    "            input_ids = batch['input_ids'].float()\n",
    "            if input_ids.shape[1] < critic_model.fc1.in_features:\n",
    "                padding = torch.zeros(input_ids.shape[0],\n",
    "                                      critic_model.fc1.in_features - input_ids.shape[1]).to(device)\n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "            elif input_ids.shape[1] > critic_model.fc1.in_features:\n",
    "                input_ids = input_ids[:, :critic_model.fc1.in_features]\n",
    "                \n",
    "            values = critic_model(input_ids)\n",
    "            advantages = (rewards - values.squeeze().detach()).unsqueeze(-1)\n",
    "            \n",
    "            # Perform a PPO training step (assume ppo_step returns the training loss for this batch)\n",
    "            training_loss_value = ppo_step(model, batch, old_log_probs, values, rewards, advantages, optimizer)\n",
    "            ppo_steps += 1\n",
    "        else:\n",
    "            # SFT training branch\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            sft_loss = outputs.loss\n",
    "            sft_loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss_value = sft_loss.item()\n",
    "        \n",
    "        running_loss += training_loss_value\n",
    "        \n",
    "        # Calculate reward: reduction in loss relative to the baseline (positive if loss decreased)\n",
    "        reward = baseline_loss - training_loss_value\n",
    "        # Update baseline using a moving average (alpha is the smoothing factor)\n",
    "        baseline_loss = alpha * baseline_loss + (1 - alpha) * training_loss_value\n",
    "        \n",
    "        # Update the controller network using a policy gradient objective (maximize reward)\n",
    "        controller_loss = -log_prob * reward  # Both log_prob and reward are scalars\n",
    "        controller_optimizer.zero_grad()\n",
    "        controller_loss.backward()\n",
    "        controller_optimizer.step()\n",
    "        \n",
    "    avg_loss = running_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Average Training Loss: {avg_loss}\")\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Validation phase (using SFT for validation)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {val_loss}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Predictions and Evaluating Code Quality with CodeBLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test data loader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=200)\n",
    "\n",
    "# Function to generate predictions from the model\n",
    "def generate_predictions(data_loader, max_length=512):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # Note the max_length parameter set to 512\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length)\n",
    "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs]\n",
    "            decoded_labels = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in batch['labels']]\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "    return predictions, references\n",
    "\n",
    "# Use the updated function to customize max_length as needed\n",
    "predictions, references = generate_predictions(test_dataloader, max_length=64)\n",
    "\n",
    "# Filter predictions and references\n",
    "predictions = predictions[:300] + predictions[401:]\n",
    "references = references[:300] + references[401:]\n",
    "\n",
    "# Calculate CodeBLEU score\n",
    "res = calc_codebleu(predictions, references, \"javascript\")\n",
    "\n",
    "# Calculate CrystalBLEU score\n",
    "res2 = corpus_bleu(references, predictions, weights=[0.1, 0.1, 0.1, 0.1], smoothing_function=SmoothingFunction().method4)\n",
    "\n",
    "print(res, res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SimplePPO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
